{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refresh imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- torch stuff ------------------------------- #\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------------- other ---------------------------------- #\n",
    "from glob import glob\n",
    "# import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "# ---------------------------------- Custom ---------------------------------- #\n",
    "from utils.precision_loss import show_sum_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why using scipy.io instead soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"soundfile converts directly to float 64\")\n",
    "# sf.read(\"../data/LibriCount/10_85b5ac.wav\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavfile concervs the original format : int16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3095,  3265,  4328, ..., -1835, -1613, -1282], dtype=int16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"wavfile concervs the original format : int16\")\n",
    "wavfile.read(\"../data/LibriCount/10_85b5ac.wav\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "F16 = torch.float16\n",
    "F32 = torch.float32\n",
    "F64 = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Errors from int16 in range(-32768, 32768) maxed by (max is=) :\n",
      "\tFloat 16  8.0\n",
      "\tFloat 32  0.0\n",
      "\tFloat 64  0.0\n"
     ]
    }
   ],
   "source": [
    "show_sum_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/LibriCount\"\n",
    "FTYPE = F32\n",
    "TRAIN_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCountGender(Dataset):\n",
    "    def __init__(self, data_dir=data_dir, dtype = FTYPE, cache=True):\n",
    "        self.sounds = glob(os.path.join(data_dir,\"*.wav\"))\n",
    "        self.labels = glob(os.path.join(data_dir,\"*.json\"))\n",
    "        self.dtype = dtype\n",
    "        self.cache = cache\n",
    "        if self.cache:\n",
    "            self.data = []\n",
    "            for index in range(len(self.sounds)):\n",
    "                sample_rate, clip = wavfile.read(self.sounds[index])\n",
    "                with open(self.labels[index]) as f:\n",
    "                    label = json.load(f)\n",
    "                genders = [0, 0] #[Male, Female]\n",
    "                for person in label:\n",
    "                    gender = person[\"sex\"]\n",
    "                    if gender == \"F\":\n",
    "                        genders[1] += 1\n",
    "                    else :\n",
    "                        genders[0] += 1\n",
    "                self.data.append([torch.tensor(clip, dtype=self.dtype).unsqueeze(0), torch.tensor(genders, dtype=self.dtype)]) #unsqueeze serves for channel = 1\n",
    "                \n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        if self.cache:\n",
    "            return self.data[index]\n",
    "        else:\n",
    "            # clip, sample_rate = sf.read(self.sounds[index])\n",
    "            sample_rate, clip = wavfile.read(self.sounds[index])\n",
    "            with open(self.labels[index]) as f:\n",
    "                label = json.load(f)\n",
    "            genders = [0, 0] #[Male, Female]\n",
    "            for person in label:\n",
    "                gender = person[\"sex\"]\n",
    "                if gender == \"F\":\n",
    "                    genders[1] += 1\n",
    "                else :\n",
    "                    genders[0] += 1\n",
    "            return torch.tensor(clip, dtype=self.dtype).unsqueeze(0), torch.tensor(genders, dtype=self.dtype) #unsqueeze serves for channel = 1\n",
    "    def __len__(self):\n",
    "        return len(self.sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AudioCountGender()\n",
    "data[800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataloader = DataLoader(dataset=data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_88_\n",
      "Duration: 0.76s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# L = []\n",
    "start = time()\n",
    "for d in dataloader:\n",
    "    # L.append(d)\n",
    "    print(len(d[0]), end=\"_\")\n",
    "end = time()\n",
    "print()\n",
    "print(\"Duration: {:.2f}s\".format(end-start))\n",
    "#Energy efficient mode\n",
    "#Duration: 55.97s for F16 and no caching and list appending\n",
    "#Duration: 1.72s for F16 and caching and list appending\n",
    "#Equilibre mode \n",
    "#Duration: 12.46s for F16 and no caching and no list appending\n",
    "#Duration: 0.88s for F16 and caching andn no list appending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch sequential\n",
    "c_1 = 64\n",
    "kernel_1 = 21\n",
    "c_2 = 64\n",
    "kernel_2 = 7\n",
    "kernel_pool = 3\n",
    "\n",
    "c_3 = 128\n",
    "kernel_3 = 3\n",
    "c_4 = 256\n",
    "kernel_4 = 3\n",
    "model = torch.nn.Sequential( #input size = 80000\n",
    "    torch.nn.Conv1d(1, c_1, kernel_1, stride=5, dtype=FTYPE),\n",
    "    torch.nn.Conv1d(c_1, c_2, kernel_2, dtype=FTYPE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.MaxPool1d(kernel_pool),\n",
    "    \n",
    "    torch.nn.Conv1d(c_2, c_3, kernel_3, dtype=FTYPE),\n",
    "    torch.nn.Conv1d(c_3, c_4, kernel_4, dtype=FTYPE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.MaxPool1d(kernel_pool),\n",
    "    \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(c_4*1775, 2, dtype=FTYPE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  torch.Size([88, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2570e+02,  5.5099e+02],\n",
       "        [-1.2534e+03,  1.3617e+02],\n",
       "        [-4.6795e+01,  7.4459e+02],\n",
       "        [-8.2248e+02,  8.5553e+02],\n",
       "        [ 3.1124e+02,  3.1932e+02],\n",
       "        [-5.9964e+02,  1.2370e+03],\n",
       "        [-3.0364e+02,  1.5586e+02],\n",
       "        [-6.4023e+02,  1.3432e+03],\n",
       "        [ 5.3624e+02, -1.4660e+02],\n",
       "        [-3.1876e+02, -8.3244e+01],\n",
       "        [-6.2716e+02, -8.6650e+02],\n",
       "        [-6.2086e+00,  9.5093e+01],\n",
       "        [-3.5492e+02, -4.7577e+02],\n",
       "        [-1.3043e+03, -3.5862e+02],\n",
       "        [-3.1223e+02, -1.3156e+02],\n",
       "        [ 1.5397e+00,  1.9312e+00],\n",
       "        [ 2.8877e+02,  1.2216e+03],\n",
       "        [-8.6916e+02,  1.0431e+03],\n",
       "        [-5.5560e+02,  7.5136e+02],\n",
       "        [-6.3094e+02,  5.6613e+02],\n",
       "        [-3.3878e+00, -2.9167e+00],\n",
       "        [-3.1200e-01, -1.0904e+01],\n",
       "        [ 1.1385e+03,  2.6615e+02],\n",
       "        [-7.8303e+02,  7.8648e+01],\n",
       "        [ 4.6866e+02,  4.2235e+02],\n",
       "        [-1.4756e+03, -1.0930e+03],\n",
       "        [-7.7822e+01,  2.3077e+02],\n",
       "        [ 3.6051e+02,  8.9263e+02],\n",
       "        [ 1.2050e+02,  9.3714e+01],\n",
       "        [ 2.0901e+02, -6.3154e+01],\n",
       "        [-1.3577e+02,  2.6352e+02],\n",
       "        [ 2.0671e+03,  1.1367e+03],\n",
       "        [-5.3799e+00, -3.0978e+00],\n",
       "        [-7.0912e+00, -9.3296e+00],\n",
       "        [-1.0405e+03,  2.2400e+02],\n",
       "        [ 7.7088e+02,  9.1863e+02],\n",
       "        [-1.5234e+02,  4.1281e+02],\n",
       "        [ 1.5462e+02, -8.3785e+02],\n",
       "        [ 1.7494e+02,  3.4367e+02],\n",
       "        [-3.1443e+02, -2.3863e+02],\n",
       "        [-3.1498e+02, -3.0972e+01],\n",
       "        [-1.4339e+02, -5.3595e+01],\n",
       "        [-3.1910e+02, -2.9370e+02],\n",
       "        [-2.2284e+02,  6.6727e+01],\n",
       "        [ 8.0866e+02, -5.2938e+02],\n",
       "        [-3.9265e-02,  4.7012e+00],\n",
       "        [ 8.8229e+01,  1.0212e+01],\n",
       "        [-1.4551e+02, -2.0301e+02],\n",
       "        [-1.1481e+03,  5.7285e+02],\n",
       "        [ 8.7128e+02,  1.2829e+03],\n",
       "        [ 2.1204e+02, -1.3194e+01],\n",
       "        [ 2.2042e+02,  5.7181e+02],\n",
       "        [ 4.2096e+02, -5.8781e+02],\n",
       "        [-9.0840e+01, -5.5984e+02],\n",
       "        [-5.1250e+02, -5.2908e+02],\n",
       "        [-2.7928e+02, -1.0536e+02],\n",
       "        [ 6.9312e+02, -8.8057e+02],\n",
       "        [ 9.5341e-01, -1.0445e+00],\n",
       "        [ 1.1681e+02,  3.6196e+02],\n",
       "        [-4.4668e+02,  5.7501e+02],\n",
       "        [ 1.7582e+02, -6.9076e+02],\n",
       "        [-3.1307e+00,  1.2495e+00],\n",
       "        [ 4.8570e+02,  7.6230e+02],\n",
       "        [ 1.7442e+02, -3.0389e+02],\n",
       "        [ 8.8967e-01,  1.4310e+00],\n",
       "        [-3.1466e+02, -2.9014e+02],\n",
       "        [ 2.4812e+02,  2.7207e+02],\n",
       "        [-9.7036e+02, -4.3625e+02],\n",
       "        [ 1.1430e+03, -3.2690e+01],\n",
       "        [-9.2275e+02,  6.7617e+01],\n",
       "        [-4.0605e+02, -1.8808e+03],\n",
       "        [-4.9878e+02, -1.9902e+02],\n",
       "        [-4.7559e+02,  1.1680e+03],\n",
       "        [ 8.9314e+01, -3.4947e+02],\n",
       "        [ 5.7111e+02,  3.5347e+02],\n",
       "        [ 9.0306e+02,  7.2496e+02],\n",
       "        [-2.3745e+02,  5.1704e+02],\n",
       "        [-1.2510e+01,  1.2772e+02],\n",
       "        [ 4.8196e-01,  6.5928e-01],\n",
       "        [-5.6029e+02,  1.0727e+02],\n",
       "        [ 1.2212e+02,  8.9093e+01],\n",
       "        [-9.9821e+02, -2.6144e+02],\n",
       "        [-2.7616e+02, -4.8924e+02],\n",
       "        [-4.4593e+02, -7.1160e+02],\n",
       "        [-5.4975e+01, -1.4156e+03],\n",
       "        [-4.1834e+02,  6.0391e+02],\n",
       "        [-8.0335e+02,  5.1314e+01],\n",
       "        [-3.7084e+02, -2.5392e+02]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = d[0].to(\"cuda\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(\"Shape : \", model.forward(example).shape)\n",
    "model.forward(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmustapha\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mustapha/klee_project_audio/runs/35nl2qm8\" target=\"_blank\">bumbling-meadow-20</a></strong> to <a href=\"https://wandb.ai/mustapha/klee_project_audio\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/mustapha/klee_project_audio/runs/35nl2qm8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f04b0185fa0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"klee_project_audio\", entity=\"mustapha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 700\n",
    "OPTIMIZER_L2 = 0.01\n",
    "OPTIMIZER_MOM = 0.9\n",
    "SCHEDULER_PATIENCE = 10\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "SCHEDULER_MIN_LR = 1e-6\n",
    "CLIP_GRAD = 1e5\n",
    "\n",
    "\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"OPTIMIZER\": \"SGD\",\n",
    "    \"OPTIMIZER_L2\":OPTIMIZER_L2,\n",
    "    \"OPTIMIZER_MOM\": OPTIMIZER_MOM,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"SCHEDULER\": \"ReduceLROnPlateau\",\n",
    "    \"SCHEDULER_PATIENCE\": SCHEDULER_PATIENCE,\n",
    "    \"SCHEDULER_FACTOR\": SCHEDULER_FACTOR,\n",
    "    \"SCHEDULER_MIN_LR\": SCHEDULER_MIN_LR,\n",
    "    \"CLIP_GRAD\": CLIP_GRAD,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=OPTIMIZER_L2, momentum=OPTIMIZER_MOM)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=SCHEDULER_PATIENCE, factor=SCHEDULER_FACTOR,min_lr=SCHEDULER_MIN_LR)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mean_loss = 0\n",
    "    for d in tqdm(dataloader):\n",
    "        audios = d[0].to(\"cuda\")\n",
    "        labels = d[1].to(\"cuda\")\n",
    "        predictions = model.forward(audios)\n",
    "        loss_value = loss(predictions, labels)\n",
    "        mean_loss += loss_value.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), -CLIP_GRAD, CLIP_GRAD)\n",
    "        optimizer.step()\n",
    "    scheduler.step(mean_loss/len(dataloader))\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, EPOCHS))\n",
    "    print(\"Train Loss : {:.4f}\".format(mean_loss/len(dataloader)))\n",
    "    wandb.log({\"loss\":mean_loss/len(dataloader), \"epoch\":epoch, \"next_lr\" :scheduler.state_dict()[\"_last_lr\"][0]})\n",
    "    # wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip, label = data[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34a01c7cbd4d230e2daaa44d8acb4ab26fe27453faf910e8270bab9ecc3bb26f"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
