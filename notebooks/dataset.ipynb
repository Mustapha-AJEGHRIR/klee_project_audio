{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refresh imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- torch stuff ------------------------------- #\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------------- other ---------------------------------- #\n",
    "from glob import glob\n",
    "# import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "# ---------------------------------- Custom ---------------------------------- #\n",
    "from utils.precision_loss import show_sum_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why using scipy.io instead soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"soundfile converts directly to float 64\")\n",
    "# sf.read(\"../data/LibriCount/10_85b5ac.wav\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavfile concervs the original format : int16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3095,  3265,  4328, ..., -1835, -1613, -1282], dtype=int16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"wavfile concervs the original format : int16\")\n",
    "wavfile.read(\"../data/LibriCount/10_85b5ac.wav\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "F16 = torch.float16\n",
    "F32 = torch.float32\n",
    "F64 = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Errors from int16 in range(-32768, 32768) maxed by (max is=) :\n",
      "\tFloat 16  8.0\n",
      "\tFloat 32  0.0\n",
      "\tFloat 64  0.0\n"
     ]
    }
   ],
   "source": [
    "show_sum_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/LibriCount\"\n",
    "FTYPE = F32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCountGender(Dataset):\n",
    "    def __init__(self, data_dir=data_dir, dtype = FTYPE, cache=True):\n",
    "        self.sounds = glob(os.path.join(data_dir,\"*.wav\"))\n",
    "        self.labels = glob(os.path.join(data_dir,\"*.json\"))\n",
    "        self.dtype = dtype\n",
    "        self.cache = cache\n",
    "        if self.cache:\n",
    "            self.data = []\n",
    "            for index in range(len(self.sounds)):\n",
    "                sample_rate, clip = wavfile.read(self.sounds[index])\n",
    "                with open(self.labels[index]) as f:\n",
    "                    label = json.load(f)\n",
    "                genders = [0, 0] #[Male, Female]\n",
    "                for person in label:\n",
    "                    gender = person[\"sex\"]\n",
    "                    if gender == \"F\":\n",
    "                        genders[1] += 1\n",
    "                    else :\n",
    "                        genders[0] += 1\n",
    "                self.data.append([torch.tensor(clip, dtype=self.dtype).unsqueeze(0), torch.tensor(genders, dtype=self.dtype)]) #unsqueeze serves for channel = 1\n",
    "                \n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        if self.cache:\n",
    "            return self.data[index]\n",
    "        else:\n",
    "            # clip, sample_rate = sf.read(self.sounds[index])\n",
    "            sample_rate, clip = wavfile.read(self.sounds[index])\n",
    "            with open(self.labels[index]) as f:\n",
    "                label = json.load(f)\n",
    "            genders = [0, 0] #[Male, Female]\n",
    "            for person in label:\n",
    "                gender = person[\"sex\"]\n",
    "                if gender == \"F\":\n",
    "                    genders[1] += 1\n",
    "                else :\n",
    "                    genders[0] += 1\n",
    "            return torch.tensor(clip, dtype=self.dtype).unsqueeze(0), torch.tensor(genders, dtype=self.dtype) #unsqueeze serves for channel = 1\n",
    "    def __len__(self):\n",
    "        return len(self.sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[4939., 3212., 1816.,  ..., 7072., 8158., 6819.]]), tensor([4., 3.])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = AudioCountGender()\n",
    "data[800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "dataloader = DataLoader(dataset=data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_128_88_\n",
      "Duration: 0.71s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# L = []\n",
    "start = time()\n",
    "for d in dataloader:\n",
    "    # L.append(d)\n",
    "    print(len(d[0]), end=\"_\")\n",
    "end = time()\n",
    "print()\n",
    "print(\"Duration: {:.2f}s\".format(end-start))\n",
    "#Energy efficient mode\n",
    "#Duration: 55.97s for F16 and no caching and list appending\n",
    "#Duration: 1.72s for F16 and caching and list appending\n",
    "#Equilibre mode \n",
    "#Duration: 12.46s for F16 and no caching and no list appending\n",
    "#Duration: 0.88s for F16 and caching andn no list appending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch sequential\n",
    "c_1 = 64\n",
    "kernel_1 = 21\n",
    "c_2 = 64\n",
    "kernel_2 = 7\n",
    "kernel_pool = 3\n",
    "\n",
    "c_3 = 128\n",
    "kernel_3 = 3\n",
    "c_4 = 256\n",
    "kernel_4 = 3\n",
    "model = torch.nn.Sequential( #input size = 80000\n",
    "    torch.nn.Conv1d(1, c_1, kernel_1, stride=5, dtype=FTYPE),\n",
    "    torch.nn.Conv1d(c_1, c_2, kernel_2, dtype=FTYPE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.MaxPool1d(kernel_pool),\n",
    "    \n",
    "    torch.nn.Conv1d(c_2, c_3, kernel_3, dtype=FTYPE),\n",
    "    torch.nn.Conv1d(c_3, c_4, kernel_4, dtype=FTYPE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.MaxPool1d(kernel_pool),\n",
    "    \n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(c_4*1775, 2, dtype=FTYPE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape :  torch.Size([88, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1549e+02, -2.8206e+02],\n",
       "        [ 2.2760e+02, -6.6087e+02],\n",
       "        [-6.1310e+02, -1.0468e+03],\n",
       "        [ 2.4236e+02, -1.1986e+03],\n",
       "        [-1.8255e+03, -9.6716e+02],\n",
       "        [-3.8826e+02, -1.0930e+03],\n",
       "        [-2.0330e+02, -1.1252e+03],\n",
       "        [-7.9902e+02, -8.9182e+01],\n",
       "        [ 1.8814e+02, -1.5422e+03],\n",
       "        [ 4.8973e+02, -1.0361e+03],\n",
       "        [ 4.2772e+02, -6.6013e+02],\n",
       "        [-4.6660e+02,  1.7635e+02],\n",
       "        [ 1.2969e+03, -1.4697e+02],\n",
       "        [-5.7303e+02, -1.1287e+03],\n",
       "        [-1.8187e+01, -2.1688e+01],\n",
       "        [ 8.1732e+01, -2.2329e+02],\n",
       "        [-1.3398e+02, -1.2746e+03],\n",
       "        [-9.0785e+02, -1.1192e+03],\n",
       "        [-7.1595e+02, -3.9706e+02],\n",
       "        [-4.3094e+01, -2.1610e+03],\n",
       "        [ 2.3158e+02, -3.4770e+02],\n",
       "        [ 1.5884e+02, -3.3087e+02],\n",
       "        [ 2.3461e+00, -6.8877e+00],\n",
       "        [-3.1702e+02,  4.7187e+02],\n",
       "        [ 3.5908e+02, -6.4548e+02],\n",
       "        [ 2.0068e+02, -6.3367e+02],\n",
       "        [-6.1681e+02, -1.0074e+03],\n",
       "        [-3.2023e+02,  3.2490e+02],\n",
       "        [ 4.2394e+01,  1.9362e+02],\n",
       "        [ 4.8032e+02, -6.8522e+02],\n",
       "        [ 6.3355e+01, -8.3269e+01],\n",
       "        [ 1.6178e+01,  7.1061e+00],\n",
       "        [ 2.6327e+02, -1.9221e+02],\n",
       "        [-1.6600e+02,  7.5916e+01],\n",
       "        [-4.7715e+02,  6.3981e+01],\n",
       "        [-4.0903e+02, -3.1624e+02],\n",
       "        [ 4.3472e+02, -3.0133e+02],\n",
       "        [-2.2306e+02, -9.9210e+01],\n",
       "        [-9.6824e+02, -4.3092e+01],\n",
       "        [ 5.3323e+02, -4.8283e+02],\n",
       "        [-4.7240e+01, -1.0984e+03],\n",
       "        [-2.8504e+02, -4.3853e+02],\n",
       "        [-1.4036e+02,  1.3297e+03],\n",
       "        [-2.9819e+01, -5.1493e+00],\n",
       "        [-4.0635e+02,  1.6555e+02],\n",
       "        [ 5.2742e+02, -1.3069e+03],\n",
       "        [-4.2258e+02, -1.6966e+02],\n",
       "        [-8.8880e+02, -3.3255e+02],\n",
       "        [-1.1841e+03, -1.0121e+03],\n",
       "        [-8.3002e+02, -5.8864e+02],\n",
       "        [-4.4749e+02, -1.2525e+03],\n",
       "        [ 6.5282e+02, -8.7692e+02],\n",
       "        [-4.7193e+01,  4.8008e+02],\n",
       "        [ 2.6157e+00, -2.3398e+00],\n",
       "        [-7.5627e+02, -7.3096e+02],\n",
       "        [-4.3933e+02, -3.5269e+02],\n",
       "        [ 9.0046e+01, -1.1622e+03],\n",
       "        [ 9.2174e-01, -4.2102e+00],\n",
       "        [-6.7193e+01, -1.6189e+01],\n",
       "        [-5.9723e+01, -2.8282e+02],\n",
       "        [-3.6798e-01, -8.0437e+00],\n",
       "        [ 2.9513e+02, -7.2169e+02],\n",
       "        [-2.5944e+02, -4.4802e+02],\n",
       "        [-1.5122e+02, -5.7911e+00],\n",
       "        [-1.2892e+02, -8.6913e+02],\n",
       "        [ 4.5180e+02, -2.0201e+02],\n",
       "        [ 7.3850e+01, -1.6643e+02],\n",
       "        [ 1.8419e+01, -1.0412e+02],\n",
       "        [-3.9973e+02, -5.8930e+02],\n",
       "        [ 8.5334e+02, -3.5813e+02],\n",
       "        [-2.1434e+02, -3.2982e+02],\n",
       "        [-5.8686e+02, -5.5348e+02],\n",
       "        [-7.3228e+02, -4.0865e+02],\n",
       "        [ 3.6463e+02, -3.8581e+01],\n",
       "        [-6.8134e+02, -3.0439e+02],\n",
       "        [-9.7165e+02, -1.3372e+03],\n",
       "        [ 2.5974e+02, -1.0814e+03],\n",
       "        [ 2.1981e+02,  1.0498e+02],\n",
       "        [ 3.8954e+02, -5.1489e+02],\n",
       "        [ 1.0676e+00,  1.3335e+00],\n",
       "        [-7.0289e+02, -1.0435e+03],\n",
       "        [-5.4679e+02, -2.8653e+02],\n",
       "        [ 1.6615e+00,  1.1442e+01],\n",
       "        [ 8.9637e+02, -7.8505e+02],\n",
       "        [ 1.1514e+00,  2.8283e+00],\n",
       "        [-1.1883e+03, -5.2123e+02],\n",
       "        [-1.3513e+03, -1.1636e+03],\n",
       "        [ 2.2868e+02,  1.0291e+02]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = d[0].to(\"cuda\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(\"Shape : \", model.forward(example).shape)\n",
    "model.forward(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmustapha\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mustapha/klee_project_audio/runs/gw0gahim\" target=\"_blank\">mild-puddle-10</a></strong> to <a href=\"https://wandb.ai/mustapha/klee_project_audio\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/mustapha/klee_project_audio/runs/gw0gahim?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8bc0d138e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"klee_project_audio\", entity=\"mustapha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 700\n",
    "SCHEDULER_PATIENCE = 5\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "SCHEDULER_MIN_LR = 1e-5\n",
    "\n",
    "\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"SCHEDULER\": \"ReduceLROnPlateau\",\n",
    "    \"SCHEDULER_PATIENCE\": SCHEDULER_PATIENCE,\n",
    "    \"SCHEDULER_FACTOR\": SCHEDULER_FACTOR,\n",
    "    \"SCHEDULER_MIN_LR\": SCHEDULER_MIN_LR,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "Train Loss : 1844939127.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/700\n",
      "Train Loss : 10.6235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/700\n",
      "Train Loss : 10.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/700\n",
      "Train Loss : 10.6340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/700\n",
      "Train Loss : 10.5172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:18<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/700\n",
      "Train Loss : 10.5301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:18<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/700\n",
      "Train Loss : 10.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:18<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/700\n",
      "Train Loss : 10.5504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 20/45 [00:08<00:10,  2.46it/s]"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=SCHEDULER_PATIENCE, factor=SCHEDULER_FACTOR,min_lr=SCHEDULER_MIN_LR)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mean_loss = 0\n",
    "    for d in tqdm(dataloader):\n",
    "        audios = d[0].to(\"cuda\")\n",
    "        labels = d[1].to(\"cuda\")\n",
    "        predictions = model.forward(audios)\n",
    "        loss_value = loss(predictions, labels)\n",
    "        mean_loss += loss_value.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step(mean_loss/len(dataloader))\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, EPOCHS))\n",
    "    print(\"Train Loss : {:.4f}\".format(mean_loss/len(dataloader)))\n",
    "    wandb.log({\"loss\":mean_loss/len(dataloader), \"epoch\":epoch, \"next_lr\" :scheduler.state_dict()[\"_last_lr\"][0]})\n",
    "    # wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34a01c7cbd4d230e2daaa44d8acb4ab26fe27453faf910e8270bab9ecc3bb26f"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
